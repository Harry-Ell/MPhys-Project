{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7410b33b-2343-4fc8-b449-e4ecb5e9c039",
   "metadata": {},
   "source": [
    "# Final Collocation with SEVIRI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11201ff8-8303-4ccf-9f5e-ad54e29cd063",
   "metadata": {},
   "source": [
    "This will be a 2 step process. Function 1 will be taking all of the advected files and removing all entries which are not integer multiples of 15 minutes, as we cannot make retrievals for these times anyway. These files are lifted from \n",
    "1. '/gws/nopw/j04/eo_shared_data_vol2/scratch/pete_nut/harry_advected/RealData/')\n",
    "2. '/gws/nopw/j04/eo_shared_data_vol2/scratch/pete_nut/harry_advected/FittedData/')\n",
    "3. '/gws/nopw/j04/eo_shared_data_vol2/scratch/pete_nut/harry_advected/UpperCounterFacData/')\n",
    "4. '/gws/nopw/j04/eo_shared_data_vol2/scratch/pete_nut/harry_advected/LowerCounterFacData/')\n",
    "\n",
    "with each being followed by 2019_{}_{}_{}:{}'.format(str(date.month).zfill(2), str(date.day).zfill(2), str(date.hour).zfill(2), str(date.minute).zfill(2))) \n",
    "\n",
    "And they are trimmed, and then returned to the locations at \n",
    "1. '/gws/nopw/j04/eo_shared_data_vol2/scratch/AO12/RealDataAdvected/'\n",
    "2. '/gws/nopw/j04/eo_shared_data_vol2/scratch/AO12/FittedlDataAdvected/'\n",
    "3. '/gws/nopw/j04/eo_shared_data_vol2/scratch/AO12/UpperCounterFacDataAdvected/'\n",
    "4. '/gws/nopw/j04/eo_shared_data_vol2/scratch/AO12/LowerCounterFacDataAdvected/'\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd43abc-0814-4309-a27e-684a07a84092",
   "metadata": {},
   "source": [
    "Function 1 below deals with this process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08acaad-b266-4971-bc44-a7603c881b8c",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a460a35b-a2ad-4d4b-9036-ca9ae1239d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pathlib\n",
    "from datetime import datetime, timedelta, time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from scipy.spatial import KDTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46dac50-3c9e-4d85-926b-6b5d9073586e",
   "metadata": {},
   "source": [
    "## Function 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89aa27be-a8c1-4f94-ac4d-044c49bc533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MissingFiles = []\n",
    "\n",
    "# def TrimAndCopy(StartDate, EndDate, FileType):\n",
    "#     Link = f'/gws/nopw/j04/eo_shared_data_vol2/scratch/pete_nut/harry_advected/{FileType}Data'\n",
    "    \n",
    "#     while StartDate < EndDate:\n",
    "#         try:\n",
    "#             L = Link + f\"/{StartDate.year:04d}_{StartDate.month:02d}_{StartDate.day:02d}_{StartDate.hour:02d}:{StartDate.minute:02d}\"\n",
    "#             File1 = pd.read_csv(L)\n",
    "            \n",
    "#             # Filter based on latitude and longitude\n",
    "#             File1 = File1[(File1['lat'] >= -22) & (File1['lat'] <= 0) & (File1['lon'] >= -16) & (File1['lon'] <= 10)]\n",
    "            \n",
    "#             File1['jday'] = pd.to_datetime(File1['jday'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "            \n",
    "#             PlaceholderName = File1[File1['jday'].dt.minute % 15 == 0]\n",
    "            \n",
    "#             PlaceholderName.to_csv('/gws/nopw/j04/eo_shared_data_vol2/scratch/AO12/{}DataAdvected/{}_{}_{}_{}:{}'.format(\n",
    "#                 FileType, str(StartDate.year), str(StartDate.month).zfill(2), str(StartDate.day).zfill(2),\n",
    "#                 str(StartDate.hour).zfill(2), str(StartDate.minute).zfill(2)))\n",
    "            \n",
    "#             print(f\"Date: {StartDate}, Link = {'/gws/nopw/j04/eo_shared_data_vol2/scratch/AO12/{}DataAdvected/{}_{}_{}_{}:{}'.format(FileType, str(StartDate.year), str(StartDate.month).zfill(2), str(StartDate.day).zfill(2),str(StartDate.hour).zfill(2), str(StartDate.minute).zfill(2))}, Complete\")\n",
    "            \n",
    "#             StartDate += timedelta(hours=1)\n",
    "#         except FileNotFoundError:\n",
    "#             MissingFiles.append(f'{StartDate}')\n",
    "#             print(f'File for {StartDate} is missing')\n",
    "#             StartDate += timedelta(hours=1)\n",
    "\n",
    "#Heres what this is doing in words: take any given file from the Link, and it is made up of a real measurement for each particle, \n",
    "#which is then advected every 5 minutes. As SEVIRI only has 15 minute resolution, I take these copies and I move them to another location, and along the\n",
    "# way i get rid of every time measurement that will not end up being relevant, and all long- lat pairings which are not useful to our experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "766f0cd4-3cd8-4964-a50a-04f778667647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrimAndCopy(StartDate = datetime(2019, 1, 1, 0, 0), EndDate = datetime(2019, 2, 1, 0, 0), FileType = 'Real')\n",
    "# FileType = ['Real', 'Fitted', 'UpperCounterFac', 'LowerCounterFac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cc596c8-a670-4fdb-a5af-2f871c1bf616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = pd.read_csv('/gws/nopw/j04/eo_shared_data_vol2/scratch/AO12/RealDataAdvected/2019_01_01_00:00')                  \n",
    "# file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ced9f67-9ef2-4377-8263-7544d16ee2a8",
   "metadata": {},
   "source": [
    "## Function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c370c9eb-2940-4b58-8db1-0258df291de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CollocationFunction(StartDate, EndDate, FileType):\n",
    "    while StartDate < EndDate:\n",
    "        d = StartDate\n",
    "        print(d)\n",
    "        try:\n",
    "            TrimmedAdvectedEmissions = pd.read_csv(f'/gws/nopw/j04/eo_shared_data_vol2/scratch/AO12/{FileType}DataAdvected/{StartDate.year:04d}_{StartDate.month:02d}_{StartDate.day:02d}_{StartDate.hour:02d}:{StartDate.minute:02d}') \n",
    "            TrimmedAdvectedEmissions['jday'] = pd.to_datetime(TrimmedAdvectedEmissions['jday'])\n",
    "            if 'cer' in TrimmedAdvectedEmissions.columns:\n",
    "                print(f'Data already collocated for {StartDate}')\n",
    "                StartDate += timedelta(hours = 1)\n",
    "                continue\n",
    "            grouped_data = TrimmedAdvectedEmissions.groupby('jday')\n",
    "            subsets = [group for _, group in grouped_data]\n",
    "            new_groups = []\n",
    "            \n",
    "            while d < StartDate + timedelta(hours = 24, minutes = 1): # Increment date here by 1 in hour column\n",
    "                try:\n",
    "                    newPath = f'/gws/nopw/j04/eo_shared_data_vol1/satellite/seviri-orac/Data/{d.year:04d}/{d.month:02d}/{d.day:02d}/{d.hour:02d}/{d.year:04d}{d.month:02d}{d.day:02d}{d.hour:02d}{d.minute:02d}{d.second:02d}-ESACCI-L2_CLOUD-CLD_PRODUCTS-SEVIRI-MSG4-fv3.0.nc'\n",
    "                    cloud_ds = xr.open_dataset(newPath, decode_times=False)\n",
    "                \n",
    "                    wh_seviri_finite = np.isfinite(cloud_ds.lon.data.ravel())\n",
    "                    seviri_lonlat_tree = KDTree(np.stack([cloud_ds.lon.data.ravel()[wh_seviri_finite], cloud_ds.lat.data.ravel()[wh_seviri_finite]], -1))\n",
    "                    \n",
    "                    modified_fitted_group = None\n",
    "                    \n",
    "                    for group in subsets:\n",
    "                        time_difference = abs(group['jday'].iloc[0] - d)\n",
    "                        if time_difference < timedelta(minutes = 5): \n",
    "                            wh_emissions_finite = np.isfinite(group.lon.to_numpy())\n",
    "                            dists, nearest_neighbours = seviri_lonlat_tree.query(np.stack([\n",
    "                                group.lon.to_numpy()[wh_emissions_finite], \n",
    "                                group.lat.to_numpy()[wh_emissions_finite]\n",
    "                            ], -1))\n",
    "                          \n",
    "                            wh_emissions_valid = wh_emissions_finite[~np.isnan(wh_emissions_finite)]\n",
    "                            \n",
    "                            # Retrieve all desired variables from the cloud_ds data frame\n",
    "                            extracted_values1  = cloud_ds.lat.data.ravel()[wh_seviri_finite][nearest_neighbours]\n",
    "                            extracted_values2  = cloud_ds.lon.data.ravel()[wh_seviri_finite][nearest_neighbours] \n",
    "                            extracted_values3  = cloud_ds.cot.data.ravel()[wh_seviri_finite][nearest_neighbours]\n",
    "                            extracted_values4  = cloud_ds.cot_uncertainty.data.ravel()[wh_seviri_finite][nearest_neighbours]                   \n",
    "                            extracted_values5  = cloud_ds.cer.data.ravel()[wh_seviri_finite][nearest_neighbours]\n",
    "                            extracted_values6  = cloud_ds.cer_uncertainty.data.ravel()[wh_seviri_finite][nearest_neighbours]                    \n",
    "                            extracted_values7  = cloud_ds.cwp.data.ravel()[wh_seviri_finite][nearest_neighbours]\n",
    "                            extracted_values8  = cloud_ds.cwp_uncertainty.data.ravel()[wh_seviri_finite][nearest_neighbours]\n",
    "                            extracted_values9  = cloud_ds.cth.data.ravel()[wh_seviri_finite][nearest_neighbours]\n",
    "                            extracted_values10 = cloud_ds.cth_uncertainty.data.ravel()[wh_seviri_finite][nearest_neighbours]\n",
    "                            extracted_values11 = cloud_ds.illum.data.ravel()[wh_seviri_finite][nearest_neighbours]\n",
    "        \n",
    "                            # Update a copy of the group\n",
    "                            modified_fitted_group = group.copy()\n",
    "                            modified_fitted_group['pixel lat'] = np.nan                \n",
    "                            modified_fitted_group['pixel lon'] = np.nan\n",
    "                            modified_fitted_group['cot'] = np.nan\n",
    "                            modified_fitted_group['cot_uncertainty'] = np.nan                \n",
    "                            modified_fitted_group['cer'] = np.nan\n",
    "                            modified_fitted_group['cer_uncertainty'] = np.nan\n",
    "                            modified_fitted_group['cwp'] = np.nan                \n",
    "                            modified_fitted_group['cwp_uncertainty'] = np.nan\n",
    "                            modified_fitted_group['cth'] = np.nan\n",
    "                            modified_fitted_group['cth_uncertainty'] = np.nan                \n",
    "                            modified_fitted_group['illum'] = np.nan\n",
    "        \n",
    "                            \n",
    "            \n",
    "                            modified_fitted_group.loc[wh_emissions_valid, 'pixel lat'] = extracted_values1\n",
    "                            modified_fitted_group.loc[wh_emissions_valid, 'pixel lon'] = extracted_values2                \n",
    "                            modified_fitted_group.loc[wh_emissions_valid, 'cot'] = extracted_values3\n",
    "                            modified_fitted_group.loc[wh_emissions_valid, 'cot_uncertainty'] = extracted_values4\n",
    "                            modified_fitted_group.loc[wh_emissions_valid, 'cer'] = extracted_values5\n",
    "                            modified_fitted_group.loc[wh_emissions_valid, 'cer_uncertainty'] = extracted_values6\n",
    "                            modified_fitted_group.loc[wh_emissions_valid, 'cwp'] = extracted_values7\n",
    "                            modified_fitted_group.loc[wh_emissions_valid, 'cwp_uncertainty'] = extracted_values8\n",
    "                            modified_fitted_group.loc[wh_emissions_valid, 'cth'] = extracted_values9\n",
    "                            modified_fitted_group.loc[wh_emissions_valid, 'cth_uncertainty'] = extracted_values10\n",
    "                            modified_fitted_group.loc[wh_emissions_valid, 'illum'] = extracted_values11\n",
    "            \n",
    "                            \n",
    "                            # Append the modified group to the new list\n",
    "                            new_groups.append(modified_fitted_group)\n",
    "                            \n",
    "                    print(d)\n",
    "                    d += timedelta(minutes=15)\n",
    "                    \n",
    "                except FileNotFoundError:\n",
    "                   # print('no data available for ' + str(d))\n",
    "                    # Create a new group with NaN values\n",
    "                    new_group = pd.DataFrame(columns=TrimmedAdvectedEmissions.columns)\n",
    "                    new_group['jday'] = [d] * len(new_group)\n",
    "                    new_group['cot'] = np.nan\n",
    "                    new_group['cot_uncertainty'] = np.nan\n",
    "                    new_groups.append(new_group)\n",
    "                    d += timedelta(minutes=15)\n",
    "        except FileNotFoundError:\n",
    "            print(f'No data Available to collocate with for {StartDate}')\n",
    "        \n",
    "        # Concatenate the modified groups into a new DataFrame\n",
    "        new_trimmed_advected_emissions = pd.concat(new_groups, ignore_index=True)\n",
    "        #print(new_trimmed_advected_emissions)\n",
    "        new_trimmed_advected_emissions.to_csv('/gws/nopw/j04/eo_shared_data_vol2/scratch/AO12/{}DataAdvected/{}_{}_{}_{}:{}'.format(FileType, str(StartDate.year), str(StartDate.month).zfill(2), str(StartDate.day).zfill(2), str(StartDate.hour).zfill(2), str(StartDate.minute).zfill(2))) \n",
    "        print(f'Data for {StartDate} added')\n",
    "        StartDate += timedelta(hours = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e50064a2-b720-49a9-91af-c857cd12d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CollocationFunction(StartDate = datetime(2019, 1, 1, 0, 0), EndDate = datetime(2019, 1, 1, 3, 1), FileType = 'Real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87351af5-46e3-44ef-941b-e0fb4578004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "start_dates = []\n",
    "end_dates   = []\n",
    "file_types  = []\n",
    "if __name__ == '__main__':\n",
    "    # Define your input parameters\n",
    "    \n",
    "    for integer in range(0, 30):\n",
    "        start_dates.append(datetime(2019, 1, 1, 0, 0) + integer * timedelta(days = 1) ) \n",
    "        end_dates.append(datetime(2019, 1, 1, 6, 1)  + integer * timedelta(days = 1) )\n",
    "        file_types.append('Real')\n",
    "\n",
    "    # Combine the input parameters into a list of tuples\n",
    "    input_parameters = list(zip(start_dates, end_dates, file_types))\n",
    "\n",
    "    with multiprocessing.Pool(processes=10) as pool:\n",
    "        # Use starmap to pass multiple arguments to the function\n",
    "        results = pool.starmap(CollocationFunction, input_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94633c23-59c6-42f2-9c0a-e0c138f7d1fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93b5cd57-609f-4e2e-b9e6-8aae90727410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = os.listdir('/gws/nopw/j04/eo_shared_data_vol2/scratch/AO12/RealDataAdvected/')  # STARTED AT 11:50\n",
    "# for item in file:\n",
    "#     print(item)\n",
    "# item = pd.read_csv('/gws/nopw/j04/eo_shared_data_vol2/scratch/AO12/RealDataAdvected/2019_01_01_00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0d178a1-0b55-48d3-9e4a-a9600b9a5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = pd.read_csv('/gws/nopw/j04/eo_shared_data_vol2/scratch/AO12/RealDataAdvected/2019_01_01_00:00')\n",
    "# file.sort_values(by = 'cot' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97e8dee-064f-4344-b7b4-c2f7e70bd0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = pd.read_csv('/gws/nopw/j04/eo_shared_data_vol2/scratch/pete_nut/harry_advected/RealData/2019_01_18_13:00')\n",
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a321f80d-f9c8-4f0b-9518-a453293ebb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime, timedelta\n",
    "# import os\n",
    "# List = []\n",
    "# StartDate = datetime(2019, 1, 1, 0, 0)\n",
    "\n",
    "# while StartDate < datetime(2019, 2, 1, 0, 1):\n",
    "#     try:\n",
    "#         file_path = '/gws/nopw/j04/eo_shared_data_vol2/scratch/AO12/RealDataAdvected/{}_{}_{}_{}:{}'.format(\n",
    "#             str(StartDate.year), str(StartDate.month).zfill(2), str(StartDate.day).zfill(2),\n",
    "#             str(StartDate.hour).zfill(2), str(StartDate.minute).zfill(2)\n",
    "#         )\n",
    "        \n",
    "#         # Check if file exists and has a non-zero size\n",
    "#         if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "#             print(f'The date {StartDate} has a valid file at {file_path} with size {os.path.getsize(file_path)} bytes')\n",
    "#             if os.path.getsize(file_path) < 1000:\n",
    "#                 List.append(str(StartDate))\n",
    "#         else:\n",
    "#             print(f'The date {StartDate} has no valid file or has zero size')\n",
    "#     except FileNotFoundError:\n",
    "#         print('n')\n",
    "#     StartDate += timedelta(hours=1)\n",
    "# print(List)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6f1bf-1a01-4d5b-8e26-4be137c43d09",
   "metadata": {},
   "source": [
    "## Progress checks on nohup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba9664d7-ea5b-4481-8bec-df138a6eddb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of 2024-01-30 13:38:39.377245, we have completed 232 out of 653 files, and there are 92 missing files in January\n"
     ]
    }
   ],
   "source": [
    "Done  = 0\n",
    "Total = 0 \n",
    "Missing = 0\n",
    "StartDate = datetime(2019, 1, 1, 0, 0)\n",
    "while StartDate < datetime(2019, 2, 1, 0, 1):\n",
    "    try:\n",
    "        TrimmedAdvectedEmissions = pd.read_csv(f'/gws/nopw/j04/eo_shared_data_vol2/scratch/AO12/RealDataAdvected/{StartDate.year:04d}_{StartDate.month:02d}_{StartDate.day:02d}_{StartDate.hour:02d}:{StartDate.minute:02d}') \n",
    "        if 'cer' in TrimmedAdvectedEmissions.columns:\n",
    "            Done  += 1\n",
    "            Total += 1\n",
    "            StartDate += timedelta(hours = 1)\n",
    "        else:\n",
    "            Total += 1\n",
    "            StartDate += timedelta(hours = 1)\n",
    "    except FileNotFoundError:\n",
    "        Missing += 1\n",
    "        StartDate += timedelta(hours = 1)\n",
    "print(f'As of {datetime.now()}, we have completed {Done} out of {Total} files, and there are {Missing} missing files in January')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea62ffe-b89e-42a3-b280-7a797e106023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Jaspy",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
