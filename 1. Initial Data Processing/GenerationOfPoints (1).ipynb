{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810b1a35-63a4-42d5-94fb-4cfef878ed87",
   "metadata": {},
   "source": [
    "# Final Generation of Sampling Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267d4f9c-edf8-4233-ad19-bfef3baba2da",
   "metadata": {},
   "source": [
    "This function is where we take collection of sampling points which are found from emissions data, and we perfrom:\n",
    "1.  Fit the data, remove values from the set which are below a threshold length and $R^2$ value\n",
    "2.  Fit with least squares regression, forming sampling points which follow the line/ our best estimate of where it is, and parallel ship tracks along the path\n",
    "3.  Rejoin the data set together, with actual points, fitted points, and counterfactuals sorted and grouped by particle number. \n",
    "4. Run for every day in 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec89187-b81a-4111-b30a-c5753c33ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from datetime import datetime, timedelta, time\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import calendar\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "R_Squared = 0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0217d3e-db6e-407f-9df2-9cd74b196d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataCleaningAndAssignment(df, date): #This will take an input of a dataset, which we must save as df. \n",
    "    SortedParticles = df.sort_values(by = \"particle\")\n",
    "    grouped_data = SortedParticles.groupby('particle')\n",
    "    subsets = [group for _, group in grouped_data]\n",
    "    #We now complete the data cleaning. This involves removing all tracks of length less than 10, and all which do not meet a minimum R**2 value. \n",
    "    for subset in subsets:\n",
    "        subset.dropna(inplace=True) #removing any non numerical values\n",
    "        if len(subset) >= 10:\n",
    "            Xvar = subset['longitude'].values\n",
    "            Yvar = subset['latitude'].values\n",
    "            if len(Xvar) == 0:\n",
    "                continue\n",
    "            try: \n",
    "                coefficients = np.polyfit(Xvar, Yvar, 2)\n",
    "# Calculate the predicted values using the polynomial fit\n",
    "                y_pred = np.polyval(coefficients, Xvar)\n",
    "# Calculate the correlation coefficient (R-squared)\n",
    "                correlation_matrix = np.corrcoef(Yvar, y_pred)\n",
    "                r_squared = correlation_matrix[0, 1] ** 2  # Squared to get R-squared\n",
    "                subset['R**2 value'] = r_squared \n",
    "                subset['X =?'] = 'Long' \n",
    "            except Exception as e:\n",
    "            # Handle the LinAlgError here\n",
    "                print(f\"LinAlgError: {e}. Skipping subset {subset['particle'].iloc[0]}.\")\n",
    "                subset['R**2 value'] = 0  \n",
    "                pass\n",
    "            if r_squared < R_Squared:                 #If the line is of a low quality, then we rotate and see if y vs x gives better data. \n",
    "                Xvar = subset['latitude'].values\n",
    "                Yvar = subset['longitude'].values\n",
    "                if len(Xvar) == 0:\n",
    "                    continue\n",
    "                coefficients = np.polyfit(Xvar, Yvar, 2)\n",
    "# Calculate the predicted values using the polynomial fit\n",
    "                y_pred = np.polyval(coefficients, Xvar)\n",
    "# Calculate the correlation coefficient (R-squared)\n",
    "                correlation_matrix = np.corrcoef(Yvar, y_pred)\n",
    "                r_squared = correlation_matrix[0, 1] ** 2  # Squared to get R-squared\n",
    "                subset['R**2 value'] = r_squared \n",
    "                if r_squared > 0.97:\n",
    "                    subset['X =?'] = 'Lat'  \n",
    "                else:\n",
    "                    subset['X =?'] = 'neither' \n",
    "\n",
    "        else:\n",
    "            subset['R**2 value'] = 0\n",
    "            continue\n",
    "# This is where box 1 becomes box 2 in the origional script            \n",
    "# \n",
    "    Spacing = 0.27\n",
    "    for subset in subsets:\n",
    "        subset.sort_values(by='hour', inplace=True)\n",
    "        if subset.iloc[1][-2] >= 0.97 and subset.iloc[0][-1] == 'Long':\n",
    "            Xvar = subset['longitude'].values\n",
    "            Yvar = subset['latitude'].values\n",
    "            coefficients = np.polyfit(Xvar, Yvar, 2)\n",
    "            Xplotted = np.linspace(Xvar.min(), Xvar.max(), len(subset))\n",
    "            Yplotted = []\n",
    "            for i in range(len(subset)):\n",
    "                Yplotted.append(coefficients[0]*Xplotted[i]**2 + coefficients[1]*Xplotted[i] + coefficients[2])\n",
    "            if subset.iloc[0]['longitude'] <= subset.iloc[-1]['longitude']: #If the ship is moving eastwards (i.e.if last entry in long is greater than first)\n",
    "                subset['X Plotted Values'] = Xplotted\n",
    "                subset['Y Plotted Values'] = Yplotted\n",
    "            else:\n",
    "                subset['X Plotted Values'] = Xplotted[::-1]\n",
    "                subset['Y Plotted Values'] = Yplotted[::-1]           \n",
    "    #here we are now using our previous function\n",
    "            SlopeVector = []\n",
    "            for i in range(len(Xplotted)-1):\n",
    "                SlopeVector.append((Yplotted[i+1]-Yplotted[i])/(Xplotted[i+1]-Xplotted[i]))\n",
    "            SlopeVector.append(SlopeVector[-1]) #we imagine to find the last point that the ship continues on its last heading. \n",
    "    #Given the slope, we now want to move outwards orthogonally to this. The direction of step at each point is given by m\n",
    "            m = []\n",
    "            for i in range(len(SlopeVector)):\n",
    "                m.append(-SlopeVector[i]**-1)  #Taking advantage of simple euclidean geometry\n",
    "            Upper_Long = []\n",
    "            Upper_Lat  = []\n",
    "            Lower_Long = []\n",
    "            Lower_Lat  = []\n",
    "            for i in range(len(Xplotted)):\n",
    "                Upper_Long.append(Xplotted[i] + Spacing*(1 / np.sqrt(1 + m[i]**2)))\n",
    "                Upper_Lat.append(Yplotted[i] + Spacing*m[i] / np.sqrt(1 + m[i]**2))\n",
    "                Lower_Long.append(Xplotted[i] - Spacing*(1 / np.sqrt(1 + m[i]**2)))\n",
    "                Lower_Lat.append(Yplotted[i] - Spacing*m[i] / np.sqrt(1 + m[i]**2))\n",
    "    \n",
    "            if subset.iloc[0]['longitude'] <= subset.iloc[-1]['longitude']: #i.e., if the ship is moving east (right)\n",
    "                subset['X Imaginary Upper'] = Upper_Long\n",
    "                subset['Y Imaginary Upper'] = Upper_Lat\n",
    "                subset['X Imaginary Lower'] = Lower_Long\n",
    "                subset['Y Imaginary Lower'] = Lower_Lat     \n",
    "            else:\n",
    "                subset['X Imaginary Upper'] = Upper_Long[::-1]\n",
    "                subset['Y Imaginary Upper'] = Upper_Lat[::-1]\n",
    "                subset['X Imaginary Lower'] = Lower_Long[::-1]\n",
    "                subset['Y Imaginary Lower'] = Lower_Lat[::-1]\n",
    "            \n",
    "            #    subset['X Plotted Values'] = Xplotted[::-1]\n",
    "             #   subset['Y Plotted Values'] = Yplotted[::-1]           \n",
    "    #here we are now using our previous function\n",
    "            SlopeVector = []\n",
    "            for i in range(len(Xplotted)-1):\n",
    "                SlopeVector.append((Yplotted[i+1]-Yplotted[i])/(Xplotted[i+1]-Xplotted[i]))\n",
    "            SlopeVector.append(SlopeVector[-1]) #we imagine to find the last point that the ship continues on its last heading. \n",
    "    #Given the slope, we now want to move outwards orthogonally to this. The direction of step at each point is given by m\n",
    "            m = []\n",
    "            for i in range(len(SlopeVector)):\n",
    "                m.append(-SlopeVector[i]**-1)  #Taking advantage of simple euclidean geometry\n",
    "            Upper_Long = []\n",
    "            Upper_Lat  = []\n",
    "            Lower_Long = []\n",
    "            Lower_Lat  = []\n",
    "            for i in range(len(Xplotted)):\n",
    "                Upper_Long.append(Xplotted[i] + Spacing*(1 / np.sqrt(1 + m[i]**2)))\n",
    "                Upper_Lat.append(Yplotted[i] + Spacing*m[i] / np.sqrt(1 + m[i]**2))\n",
    "                Lower_Long.append(Xplotted[i] - Spacing*(1 / np.sqrt(1 + m[i]**2)))\n",
    "                Lower_Lat.append(Yplotted[i] - Spacing*m[i] / np.sqrt(1 + m[i]**2))\n",
    "    \n",
    "            if subset.iloc[0]['longitude'] <= subset.iloc[-1]['longitude']: #i.e., if the ship is moving east (right)\n",
    "                subset['X Imaginary Upper'] = Upper_Long\n",
    "                subset['Y Imaginary Upper'] = Upper_Lat\n",
    "                subset['X Imaginary Lower'] = Lower_Long\n",
    "                subset['Y Imaginary Lower'] = Lower_Lat     \n",
    "            else:\n",
    "                subset['X Imaginary Upper'] = Upper_Long[::-1]\n",
    "                subset['Y Imaginary Upper'] = Upper_Lat[::-1]\n",
    "                subset['X Imaginary Lower'] = Lower_Long[::-1]\n",
    "                subset['Y Imaginary Lower'] = Lower_Lat[::-1]\n",
    "            \n",
    "        elif subset.iloc[1][-2] >= 0.97 and subset.iloc[0][-1] == 'Lat':\n",
    "            Xvar = subset['latitude'].values\n",
    "            Yvar = subset['longitude'].values\n",
    "            coefficients = np.polyfit(Xvar, Yvar, 2)\n",
    "            Xplotted = np.linspace(Xvar.min(), Xvar.max(), len(subset))\n",
    "            Yplotted = []\n",
    "            for i in range(len(subset)):\n",
    "                Yplotted.append(coefficients[0]*Xplotted[i]**2 + coefficients[1]*Xplotted[i] + coefficients[2])\n",
    "            if Xvar[0] <= Xvar[-1]: #If the ship is moving north\n",
    "                subset['Y Plotted Values'] = Xplotted\n",
    "                subset['X Plotted Values'] = Yplotted\n",
    "            else:\n",
    "                subset['Y Plotted Values'] = Xplotted[::-1]\n",
    "                subset['X Plotted Values'] = Yplotted[::-1]           \n",
    "    #here we are now using our previous function\n",
    "            SlopeVector = []\n",
    "            for i in range(len(Xplotted)-1):\n",
    "                SlopeVector.append((Yplotted[i+1]-Yplotted[i])/(Xplotted[i+1]-Xplotted[i]))\n",
    "            SlopeVector.append(SlopeVector[-1]) #we imagine to find the last point that the ship continues on its last heading. \n",
    "    #Given the slope, we now want to move outwards orthogonally to this. The direction of step at each point is given by m\n",
    "            m = []\n",
    "            for i in range(len(SlopeVector)):\n",
    "                m.append(-SlopeVector[i]**-1)  #Taking advantage of simple euclidean geometry\n",
    "            Upper_Long = []\n",
    "            Upper_Lat  = []\n",
    "            Lower_Long = []\n",
    "            Lower_Lat  = []\n",
    "            for i in range(len(Xplotted)):\n",
    "                Upper_Long.append(Xplotted[i] + Spacing*(1 / np.sqrt(1 + m[i]**2)))\n",
    "                Upper_Lat.append(Yplotted[i] + Spacing*m[i] / np.sqrt(1 + m[i]**2))\n",
    "                Lower_Long.append(Xplotted[i] - Spacing*(1 / np.sqrt(1 + m[i]**2)))\n",
    "                Lower_Lat.append(Yplotted[i] - Spacing*m[i] / np.sqrt(1 + m[i]**2))\n",
    "    \n",
    "            if Xvar[0] <= Xvar[-1]: #i.e., if the ship is moving east (right)\n",
    "                subset['Y Imaginary Upper'] = Upper_Long\n",
    "                subset['X Imaginary Upper'] = Upper_Lat\n",
    "                subset['Y Imaginary Lower'] = Lower_Long\n",
    "                subset['X Imaginary Lower'] = Lower_Lat     \n",
    "            else:\n",
    "                subset['Y Imaginary Upper'] = Upper_Long[::-1]\n",
    "                subset['X Imaginary Upper'] = Upper_Lat[::-1]\n",
    "                subset['Y Imaginary Lower'] = Lower_Long[::-1]\n",
    "                subset['X Imaginary Lower'] = Lower_Lat[::-1]\n",
    "\n",
    "    RealValues        = []\n",
    "    FittedValues      = []\n",
    "    UpperCounterFacs  = []\n",
    "    LowerCounterFacs = []\n",
    "    for subset in subsets:\n",
    "        if subset.iloc[0][8] > 0.97:\n",
    "            for i in range(len(subset)):\n",
    "                RealValues.append([subset.iloc[i][0], subset.iloc[i][1], subset.iloc[i][2], subset.iloc[i][3], subset.iloc[i][4], subset.iloc[i][5], subset.iloc[i][6], subset.iloc[i][7], subset.iloc[i][8], 'Real LongLat']) \n",
    "            for j in range(len(subset)):\n",
    "                FittedValues.append([subset.iloc[j][0], subset.iloc[j][1], subset.iloc[j][2], subset.iloc[j][3], subset.iloc[j][4], subset.iloc[j][5], subset.iloc[j]['Y Plotted Values'], subset.iloc[j]['X Plotted Values'], subset.iloc[j][8], 'LongLatFitted'])\n",
    "            for k in range(len(subset)):\n",
    "                UpperCounterFacs.append([subset.iloc[k][0], subset.iloc[k][1], subset.iloc[k][2], subset.iloc[k][3], subset.iloc[k][4], subset.iloc[k][5], subset.iloc[k]['Y Imaginary Upper'], subset.iloc[k]['X Imaginary Upper'], subset.iloc[k][8], 'UpperCounterfactual'])\n",
    "            for l in range(len(subset)):\n",
    "                LowerCounterFacs.append([subset.iloc[l][0], subset.iloc[l][1], subset.iloc[l][2], subset.iloc[l][3], subset.iloc[l][4], subset.iloc[l][5], subset.iloc[l]['Y Imaginary Lower'], subset.iloc[l]['X Imaginary Lower'], subset.iloc[l][8], 'LowerCounterfactual'])\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    column = ['hour', 'y', 'x', 'mass', 'signal', 'particle', 'latitude', 'longitude', 'R**2 value', 'PointLabel']\n",
    "    \n",
    "    RealData = pd.DataFrame(RealValues, columns = column)\n",
    "    FittedData = pd.DataFrame(FittedValues, columns = column) \n",
    "    UpperCounterFacData = pd.DataFrame(UpperCounterFacs, columns = column)\n",
    "    LowerCounterFacData = pd.DataFrame(LowerCounterFacs, columns = column)\n",
    "    \n",
    "    RealData.to_csv('/gws/nopw/j04/eo_shared_data_vol2/scratch/AO12/RealData/{}_{}_{}'.format(str(date.year), str(date.month).zfill(2), str(date.day).zfill(2)))\n",
    "    FittedData.to_csv('/gws/nopw/j04/eo_shared_data_vol2/scratch/AO12/FittedData/{}_{}_{}'.format(str(date.year), str(date.month).zfill(2), str(date.day).zfill(2)))\n",
    "    UpperCounterFacData.to_csv('/gws/nopw/j04/eo_shared_data_vol2/scratch/AO12/UpperCounterFacData/{}_{}_{}'.format(str(date.year), str(date.month).zfill(2), str(date.day).zfill(2)))\n",
    "    LowerCounterFacData.to_csv('/gws/nopw/j04/eo_shared_data_vol2/scratch/AO12/LowerCounterFacData/{}_{}_{}'.format(str(date.year), str(date.month).zfill(2), str(date.day).zfill(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa78583-d14a-41af-9461-ff7c4ed39d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data for 2019-09-13 00:00:00\n",
      "Processed data for 2019-09-14 00:00:00\n",
      "Processed data for 2019-09-15 00:00:00\n",
      "Processed data for 2019-09-16 00:00:00\n",
      "Processed data for 2019-09-17 00:00:00\n",
      "Processed data for 2019-09-18 00:00:00\n",
      "Processed data for 2019-09-19 00:00:00\n",
      "Processed data for 2019-09-20 00:00:00\n",
      "Processed data for 2019-09-21 00:00:00\n",
      "Processed data for 2019-09-22 00:00:00\n",
      "Processed data for 2019-09-23 00:00:00\n",
      "Processed data for 2019-09-24 00:00:00\n",
      "Processed data for 2019-09-25 00:00:00\n",
      "Processed data for 2019-09-26 00:00:00\n",
      "Processed data for 2019-09-27 00:00:00\n",
      "Processed data for 2019-09-28 00:00:00\n",
      "Processed data for 2019-09-29 00:00:00\n",
      "Processed data for 2019-09-30 00:00:00\n",
      "Processed data for 2019-10-01 00:00:00\n",
      "Processed data for 2019-10-02 00:00:00\n",
      "Processed data for 2019-10-03 00:00:00\n",
      "Processed data for 2019-10-04 00:00:00\n",
      "Processed data for 2019-10-05 00:00:00\n",
      "Processed data for 2019-10-06 00:00:00\n",
      "Processed data for 2019-10-07 00:00:00\n",
      "Processed data for 2019-10-08 00:00:00\n",
      "Processed data for 2019-10-09 00:00:00\n",
      "Processed data for 2019-10-10 00:00:00\n",
      "Processed data for 2019-10-11 00:00:00\n",
      "Processed data for 2019-10-12 00:00:00\n",
      "Processed data for 2019-10-13 00:00:00\n",
      "Processed data for 2019-10-14 00:00:00\n",
      "Processed data for 2019-10-15 00:00:00\n",
      "Processed data for 2019-10-16 00:00:00\n",
      "Processed data for 2019-10-17 00:00:00\n",
      "Processed data for 2019-10-18 00:00:00\n",
      "Processed data for 2019-10-19 00:00:00\n",
      "Processed data for 2019-10-20 00:00:00\n",
      "Processed data for 2019-10-21 00:00:00\n",
      "Processed data for 2019-10-22 00:00:00\n",
      "Processed data for 2019-10-23 00:00:00\n",
      "Processed data for 2019-10-24 00:00:00\n",
      "Processed data for 2019-10-25 00:00:00\n",
      "Processed data for 2019-10-26 00:00:00\n",
      "Processed data for 2019-10-27 00:00:00\n",
      "Processed data for 2019-10-28 00:00:00\n",
      "Processed data for 2019-10-29 00:00:00\n",
      "Processed data for 2019-10-30 00:00:00\n",
      "Processed data for 2019-10-31 00:00:00\n",
      "Processed data for 2019-11-01 00:00:00\n",
      "Processed data for 2019-11-02 00:00:00\n",
      "Processed data for 2019-11-03 00:00:00\n",
      "Processed data for 2019-11-04 00:00:00\n",
      "Processed data for 2019-11-05 00:00:00\n",
      "Processed data for 2019-11-06 00:00:00\n",
      "Processed data for 2019-11-07 00:00:00\n",
      "Processed data for 2019-11-08 00:00:00\n",
      "Processed data for 2019-11-09 00:00:00\n",
      "Processed data for 2019-11-10 00:00:00\n",
      "Processed data for 2019-11-11 00:00:00\n",
      "Processed data for 2019-11-12 00:00:00\n",
      "Processed data for 2019-11-13 00:00:00\n",
      "Processed data for 2019-11-14 00:00:00\n",
      "Processed data for 2019-11-15 00:00:00\n",
      "Processed data for 2019-11-16 00:00:00\n",
      "Processed data for 2019-11-17 00:00:00\n",
      "Processed data for 2019-11-18 00:00:00\n",
      "Processed data for 2019-11-19 00:00:00\n",
      "Processed data for 2019-11-20 00:00:00\n",
      "Processed data for 2019-11-21 00:00:00\n",
      "Processed data for 2019-11-22 00:00:00\n",
      "Processed data for 2019-11-23 00:00:00\n",
      "Processed data for 2019-11-24 00:00:00\n",
      "Processed data for 2019-11-25 00:00:00\n",
      "Processed data for 2019-11-26 00:00:00\n",
      "Processed data for 2019-11-27 00:00:00\n",
      "Processed data for 2019-11-28 00:00:00\n",
      "Processed data for 2019-11-29 00:00:00\n",
      "Processed data for 2019-11-30 00:00:00\n",
      "Processed data for 2019-12-01 00:00:00\n",
      "Processed data for 2019-12-02 00:00:00\n",
      "Processed data for 2019-12-03 00:00:00\n",
      "Processed data for 2019-12-04 00:00:00\n",
      "Processed data for 2019-12-05 00:00:00\n",
      "Processed data for 2019-12-06 00:00:00\n",
      "Processed data for 2019-12-07 00:00:00\n",
      "Processed data for 2019-12-08 00:00:00\n",
      "Processed data for 2019-12-09 00:00:00\n",
      "Processed data for 2019-12-10 00:00:00\n",
      "Processed data for 2019-12-11 00:00:00\n",
      "Processed data for 2019-12-12 00:00:00\n",
      "Processed data for 2019-12-13 00:00:00\n",
      "Processed data for 2019-12-14 00:00:00\n",
      "Processed data for 2019-12-15 00:00:00\n",
      "Processed data for 2019-12-16 00:00:00\n",
      "Processed data for 2019-12-17 00:00:00\n",
      "Processed data for 2019-12-18 00:00:00\n",
      "Processed data for 2019-12-19 00:00:00\n",
      "Processed data for 2019-12-20 00:00:00\n",
      "Processed data for 2019-12-21 00:00:00\n",
      "Processed data for 2019-12-22 00:00:00\n",
      "Processed data for 2019-12-23 00:00:00\n",
      "Processed data for 2019-12-24 00:00:00\n",
      "Processed data for 2019-12-25 00:00:00\n",
      "Processed data for 2019-12-26 00:00:00\n",
      "Processed data for 2019-12-27 00:00:00\n",
      "Processed data for 2019-12-28 00:00:00\n",
      "Processed data for 2019-12-29 00:00:00\n",
      "Processed data for 2019-12-30 00:00:00\n",
      "Processed data for 2019-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "emissions_path = pathlib.Path(\"/gws/nopw/j04/eo_shared_data_vol2/scratch/pete_nut/emissions_tracked\")\n",
    "start_date = datetime(2019, 9, 13)  # day, month, year\n",
    "end_date = datetime(2019, 12, 31)  # Adjust the end date as needed\n",
    "current_date = start_date\n",
    "\n",
    "while current_date <= end_date:\n",
    "    file_path = emissions_path / f\"{current_date.year:04d}\" / f\"{current_date.month:02d}_{current_date.day:02d}\"\n",
    "    if file_path.exists():\n",
    "        Link = pd.read_csv(file_path)\n",
    "        DataCleaningAndAssignment(Link, current_date)\n",
    "        \n",
    "        print(f\"Processed data for {current_date}\")\n",
    "    else:\n",
    "        print(f\"No data found for {current_date}\")\n",
    "\n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07249fe0-6c89-405c-8884-c0011903a2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Jaspy",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
